{"cells":[{"cell_type":"markdown","source":["This stage will take less than 5 minutes, you only need to provide a root_dir in the second cell. at the end you'll get files ending with \"no_fillers\" - thouse are files you will take to LLM and later to Montage (Stages 3 and 4 )  "],"metadata":{"id":"gYaMWFIKpPak"}},{"cell_type":"markdown","metadata":{"id":"UfhHveF0dInb"},"source":["#(!)1. Mount Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":29961,"status":"ok","timestamp":1730700047400,"user":{"displayName":"Zlata Ponirovskaya","userId":"17952244006982230869"},"user_tz":0},"id":"rBjSYqLXT_2J","outputId":"a1e3006b-03a6-4617-8c12-6ba0bf7d6dba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"UaqaJUBuWR3K"},"source":["## (!) 2. Provide root-dir in line #3\n","this will collect all transcripts autometically"]},{"cell_type":"markdown","metadata":{"id":"w-fzphG9R_0N"},"source":["–£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–º—É –∫–∞—Ç–∞–ª–æ–≥—É (–∫—É—Ä—Å–∞)"]},{"cell_type":"code","source":["#@markdown # üìÇ Course Directory Scanner\n","#@markdown Enter the root directory path and file suffix for your course files, then run this cell.\n","#@markdown ---\n","#@markdown **Root Directory Path:**\n","root_dir = \"/content/drive/MyDrive/\" #@param {type:\"string\"}\n","#@markdown **File Suffix:**\n","file_suffix = \".mp3_transcription.csv\" #@param {type:\"string\"}\n","#@markdown **File Json**\n","file_json = \"mp3_transcription.json\" #@param {type:\"string\"}\n","#@markdown ---\n","\n","import os\n","\n","# Find all subdirectories in the root directory\n","subdirectories = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n","\n","# Initialize an empty list to store the paths of all matching files\n","transcript_csv = []\n","transcript_json = []\n","\n","# Loop through each subdirectory\n","for subdir in subdirectories:\n","    # Get a list of all files in the current subdirectory\n","    files = os.listdir(subdir)\n","\n","    # Loop through each file in the current subdirectory\n","    for file in files:\n","        # Check if the file ends with the specified suffix\n","        if file.endswith(file_suffix):\n","            # Add the path of the file to the list\n","            transcript_csv.append(os.path.join(subdir, file))\n","        if file.endswith(file_json):\n","            # Add the path of the file to the list\n","            transcript_json.append(os.path.join(subdir, file))\n","\n","# Count the number of files found\n","file_count = len(transcript_csv)+len(transcript_json)\n","\n","\n","# Print the results\n","print(f\"üìä **Scan Results:**\")\n","print(f\"- Number of files found: {file_count}\")\n","print(f\"- File suffix used: {file_suffix}\")\n","print(f\"- JSON file suffix used: {file_json}\")\n","print(\"\\nüìÅ **File Paths:**\")\n","for path in transcript_csv:\n","    print(f\"- {path}\")\n","for path in transcript_json:\n","    print(f\"- {path}\")\n","\n","# To use the transcript_csv list in subsequent cells:\n","# You can now use the 'transcript_csv' variable, which contains all the file paths"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"do8L8hY9mg_H","executionInfo":{"status":"ok","timestamp":1730700087134,"user_tz":0,"elapsed":232,"user":{"displayName":"Zlata Ponirovskaya","userId":"17952244006982230869"}},"outputId":"625d560b-a9c6-4678-c723-cf620e5d357d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üìä **Scan Results:**\n","- Number of files found: 2\n","- File suffix used: .mp3_transcription.csv\n","- JSON file suffix used: mp3_transcription.json\n","\n","üìÅ **File Paths:**\n","- /content/drive/MyDrive/CIRANOFILM/Cyrano_de_Bergerac/Cyrano_De_Bergerac_512kb.mp3_transcription.csv\n","- /content/drive/MyDrive/CIRANOFILM/Cyrano_de_Bergerac/Cyrano_De_Bergerac_512kb.mp3_transcription.json\n"]}]},{"cell_type":"markdown","metadata":{"id":"Yyoy0mtjSC0B"},"source":[]},{"cell_type":"markdown","metadata":{"id":"6sm4HC9i-__h"},"source":["## (:) 3. CLEANUP UNKNOWN\n","( —á–∏—Å—Ç–∏–º –∑–∞–ø–∏—Å–∏ –æ—à–∏–±–æ–∫ –ø—Ä–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3317,"status":"ok","timestamp":1730700116784,"user":{"displayName":"Zlata Ponirovskaya","userId":"17952244006982230869"},"user_tz":0},"id":"hkxD1UDrvn-T","outputId":"aa14e8c4-9f3b-4d79-c32d-3e20d1699336"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing file: /content/drive/MyDrive/CIRANOFILM/Cyrano_de_Bergerac/Cyrano_De_Bergerac_512kb.mp3_transcription.csv\n","Original row count: 11175\n","Row count after processing: 11175\n","Columns in processed file: segment_start, segment_end, segment_text, segment_speaker, word, word_start, word_end, word_speaker\n","Processed file saved as: /content/drive/MyDrive/CIRANOFILM/Cyrano_de_Bergerac/Cyrano_De_Bergerac_512kb.mp3_transcription_processedtranscript.csv\n","-------------------------\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import os\n","\n","def process_and_merge_rows(df):\n","    result = []\n","    i = 0\n","    while i < len(df):\n","        current_row = df.iloc[i].copy()\n","\n","        if pd.isna(current_row['word_start']) or pd.isna(current_row['word_end']):\n","            sequence_start = max(0, i - 1)\n","            sequence_end = min(len(df) - 1, i + 1)\n","\n","            while (sequence_end < len(df) - 1 and\n","                   (pd.isna(df.iloc[sequence_end + 1]['word_start']) or\n","                    pd.isna(df.iloc[sequence_end + 1]['word_end']))):\n","                sequence_end += 1\n","\n","            merged_row = df.iloc[sequence_start].copy()\n","            merged_row['word'] = ' '.join(df.iloc[sequence_start:sequence_end + 1]['word'])\n","            merged_row['word_end'] = df.iloc[sequence_end]['word_end']\n","\n","            result.append(merged_row)\n","            i = sequence_end + 1\n","        else:\n","            result.append(current_row)\n","            i += 1\n","\n","    return pd.DataFrame(result)\n","\n","def process_csv(file_path):\n","    # Read CSV file\n","    df = pd.read_csv(file_path)\n","\n","    # Remove 'word_score' column if it exists\n","    df = df.drop(columns=['word_score'], errors='ignore')\n","\n","    # Replace 'Unknown' with NaN\n","    df = df.replace('Unknown', np.nan)\n","\n","    # Iterative processing until stable state\n","    while True:\n","        df_processed = process_and_merge_rows(df)\n","        if len(df_processed) == len(df):\n","            break\n","        df = df_processed\n","\n","    print(f\"Original row count: {len(df)}\")\n","    print(f\"Row count after processing: {len(df_processed)}\")\n","    print(f\"Columns in processed file: {', '.join(df_processed.columns)}\")\n","\n","    # Save processed DataFrame back to CSV\n","    base_name = os.path.basename(file_path)\n","    processed_file_path = os.path.join(os.path.dirname(file_path), f'{os.path.splitext(base_name)[0]}_processedtranscript.csv')\n","    df_processed.to_csv(processed_file_path, index=False)\n","\n","    print(f\"Processed file saved as: {processed_file_path}\")\n","\n","# Process all CSV files in the list\n","for csv_file in transcript_csv:\n","    print(f\"Processing file: {csv_file}\")\n","    process_csv(csv_file)\n","    print(\"-------------------------\")"]},{"cell_type":"markdown","metadata":{"id":"LXNfMCyoohuA"},"source":["## (:) 3.2  Cleanup continue"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1801,"status":"ok","timestamp":1730700136982,"user":{"displayName":"Zlata Ponirovskaya","userId":"17952244006982230869"},"user_tz":0},"id":"Ya1tTRx4nxKm","outputId":"e89ba150-a768-4be2-850a-26f0a7f9c910"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing file: /content/drive/MyDrive/CIRANOFILM/Cyrano_de_Bergerac/Cyrano_De_Bergerac_512kb.mp3_transcription_processedtranscript.csv\n","Final cleaned file saved as: /content/drive/MyDrive/CIRANOFILM/Cyrano_de_Bergerac/Cyrano_De_Bergerac_512kb.mp3_transcription_processedcleaned.csv\n","Number of rows before removing duplicates: 11175\n","Number of rows after removing duplicates: 11168\n","Duplicates successfully removed.\n","-------------------------\n"]}],"source":["import pandas as pd\n","import os\n","\n","def remove_duplicate_word_starts(df):\n","    result = []\n","    for i in range(len(df) - 1, -1, -1):\n","        if i == len(df) - 1 or df.iloc[i]['word_start'] != df.iloc[i+1]['word_start']:\n","            result.append(df.iloc[i])\n","    return pd.DataFrame(result[::-1])\n","\n","def process_csv(file_path):\n","    # Read the processed CSV file\n","    df = pd.read_csv(file_path)\n","\n","    # Apply the function to remove duplicate word_starts\n","    df_final = remove_duplicate_word_starts(df)\n","\n","    # Generate the new file path\n","    base_name = os.path.basename(file_path)\n","    directory = os.path.dirname(file_path)\n","    new_file_name = f\"{os.path.splitext(base_name)[0].replace('_processedtranscript', '')}_processedcleaned.csv\"\n","    final_file_path = os.path.join(directory, new_file_name)\n","\n","    # Save the final DataFrame back to CSV\n","    df_final.to_csv(final_file_path, index=False)\n","\n","    print(f\"Final cleaned file saved as: {final_file_path}\")\n","    print(f\"Number of rows before removing duplicates: {len(df)}\")\n","    print(f\"Number of rows after removing duplicates: {len(df_final)}\")\n","\n","    # Check for remaining duplicates\n","    duplicates = df_final[df_final.duplicated(subset=['word_start'], keep=False)]\n","    if len(duplicates) > 0:\n","        print(f\"Warning: {len(duplicates)} rows with duplicate word_start found.\")\n","        print(\"First few duplicates:\")\n","        print(duplicates.head())\n","    else:\n","        print(\"Duplicates successfully removed.\")\n","\n","    print(\"-------------------------\")\n","\n","# Get list of processed files\n","#root_dir = ()\n","processed_files = []\n","\n","for dirpath, dirnames, filenames in os.walk(root_dir):\n","    for filename in filenames:\n","        if filename.endswith(\".mp3_transcription_processedtranscript.csv\"):\n","            processed_files.append(os.path.join(dirpath, filename))\n","\n","# Process all processed CSV files\n","for csv_file in processed_files:\n","    print(f\"Processing file: {csv_file}\")\n","    process_csv(csv_file)"]},{"cell_type":"markdown","source":["–ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞ –æ—Å–Ω–æ–≤–Ω–∞—è –∑–∞–ø–∏—Å—å - —ç—Ç–æ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ —Ü–µ–ª—ã–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏. –ê –¥–æ —ç—Ç–æ–≥–æ - –≥–ª–∞–≤–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞ - —Å–ª–æ–≤–∞."],"metadata":{"id":"ZundpKfdyT2n"}},{"cell_type":"markdown","metadata":{"id":"Gc37w-Qn4r2a"},"source":["## (:)4 Get rid of filler words\n","–£–¥–∞–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–∑–∏—Ç–æ–≤ –ø—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ–º –∏—Ö –≤ —Ç–æ—á–∫—É. –í–æ–∑–º–æ–∂–Ω–æ –≤—ã —Ö–æ—Ç–∏—Ç–µ (!)–≥–ª—è–Ω—É—Ç—å –Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç, —á—Ç–æ–±—ã –ø–æ–ø–æ–ª–Ω–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤-–ø–∞—Ä–∞–∑–∏—Ç–æ–≤ - –±—ã–≤–∞—é—Ç —Å—é—Ä–ø—Ä–∏–∑—ã.\n","\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"RtJknQsqo_MK"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":220,"status":"ok","timestamp":1730700144080,"user":{"displayName":"Zlata Ponirovskaya","userId":"17952244006982230869"},"user_tz":0},"id":"s4xG5Y6H1ymz","outputId":"4a218e3c-569d-47cb-807e-7a8bebe3d731"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing file: /content/drive/MyDrive/CIRANOFILM/Cyrano_de_Bergerac/Cyrano_De_Bergerac_512kb.mp3_transcription_processedcleaned.csv\n","Updated file with sentences without filler words saved as: /content/drive/MyDrive/CIRANOFILM/Cyrano_de_Bergerac/Cyrano_De_Bergerac_512kb.mp3_transcription_processedcleaned_no_fillers.csv\n","Number of sentences after processing: 1598\n","-------------------------\n"]}],"source":["import os\n","import pandas as pd\n","import re\n","\n","\n","# Define filler words\n","filler_words = [\n","    '—ç–º', '–∞–º', '—Ö–º', '–Ω—É', '—ç—ç—ç', '–º–º–º', '—Ç–∞–∫', '–∫–∞–∫ –±—ã', '–∫–∞–∫–æ–π-—Ç–æ', '–∫–∞–∫–∏—Ö-—Ç–æ', '–∫–∞–∫–∏–µ-—Ç–æ', '–∫–∞–∫–∏–º–∏-—Ç–æ', '—Ç–∏–ø–∞', '–∫–æ—Ä–æ—á–µ',\n","    '–∑–Ω–∞—á–∏—Ç', '—ç—Ç–æ —Å–∞–º–æ–µ', '—Ç–æ –µ—Å—Ç—å', '—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ','–∫–∞–∫ —Å–∫–∞–∑–∞—Ç—å', '–≤ –æ–±—â–µ–º', '–≤ —Ü–µ–ª–æ–º', '–≤–æ—Ç', '—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ','–∫–∞–∫ –≥–æ–≤–æ—Ä–∏—Ç—Å—è', '—Ç–∞–∫ —Å–∫–∞–∑–∞—Ç—å', '—Å–∫–∞–∂–µ–º —Ç–∞–∫','–ø–æ —Å—É—Ç–∏', '–Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ'\n","]\n","\n","# Create regular expression pattern for filler words\n","filler_pattern = re.compile(r'\\b(?:' + '|'.join(map(re.escape, filler_words)) + r')\\b', re.IGNORECASE)\n","\n","def process_words(words):\n","    processed = []\n","    i = 0\n","    while i < len(words):\n","        current_word = words[i]['word'].lower()\n","        next_word = words[i+1]['word'].lower() if i+1 < len(words) else ''\n","        two_word_phrase = f\"{current_word} {next_word}\"\n","\n","        if filler_pattern.match(current_word) or filler_pattern.match(two_word_phrase):\n","            if filler_pattern.match(two_word_phrase):\n","                i += 1  # Skip next word if it's part of a two-word filler\n","        else:\n","            processed.append(words[i])\n","        i += 1\n","    return processed\n","\n","def group_sentences(words):\n","    sentences = []\n","    current_sentence = []\n","    sentence_start = None\n","    sentence_speaker = None\n","\n","    for i, word in enumerate(words):\n","        if sentence_start is None:\n","            sentence_start = word['word_start']\n","            sentence_speaker = word['word_speaker']\n","\n","        if word['word'].endswith('.'):\n","            current_sentence.append(word)\n","            sentences.append({\n","                'sentence_text': ' '.join([w['word'] for w in current_sentence]),\n","                'sentence_start': sentence_start,\n","                'sentence_end': word['word_end'],\n","                'sentence_speaker': sentence_speaker\n","            })\n","            current_sentence = []\n","            sentence_start = None\n","            sentence_speaker = None\n","        else:\n","            current_sentence.append(word)\n","\n","    if current_sentence:\n","        sentences.append({\n","            'sentence_text': ' '.join([w['word'] for w in current_sentence]),\n","            'sentence_start': sentence_start,\n","            'sentence_end': words[-1]['word_end'],\n","            'sentence_speaker': sentence_speaker\n","        })\n","\n","    return sentences\n","\n","def process_csv(file_path):\n","    # Read the CSV file\n","    df = pd.read_csv(file_path)\n","\n","    # Select only needed columns\n","    df = df[['word', 'word_start', 'word_end', 'word_speaker']]\n","\n","    # Process words\n","    processed_words = process_words(df.to_dict('records'))\n","\n","    # Group words into sentences\n","    sentences = group_sentences(processed_words)\n","\n","    # Create new DataFrame\n","    new_df = pd.DataFrame(sentences)\n","\n","    # Add sentence IDs\n","    new_df['sentence_id'] = range(1, len(new_df) + 1)\n","\n","    # Reorder columns\n","    columns = ['sentence_id', 'sentence_start', 'sentence_end', 'sentence_text', 'sentence_speaker']\n","    new_df = new_df[columns]\n","\n","    # Generate the new file path\n","    base_name = os.path.basename(file_path)\n","    directory = os.path.dirname(file_path)\n","    new_file_name = f\"{os.path.splitext(base_name)[0].replace('.mp3_transcription_processedcleaned.csv', '')}_no_fillers.csv\"\n","    output_file_path = os.path.join(directory, new_file_name)\n","\n","    # Save the updated DataFrame\n","    new_df.to_csv(output_file_path, index=False)\n","\n","    print(f\"Updated file with sentences without filler words saved as: {output_file_path}\")\n","    print(f\"Number of sentences after processing: {len(new_df)}\")\n","    print(\"-------------------------\")\n","\n","# Get list of processed files\n","#root_dir = ()\n","processed_files = []\n","\n","for dirpath, dirnames, filenames in os.walk(root_dir):\n","    for filename in filenames:\n","        if filename.endswith(\".mp3_transcription_processedcleaned.csv\"):\n","            processed_files.append(os.path.join(dirpath, filename))\n","\n","# Process all processed CSV files\n","for csv_file in processed_files:\n","    print(f\"Processing file: {csv_file}\")\n","    process_csv(csv_file)"]},{"cell_type":"markdown","source":["# **`! Don't forget to disconnect GPU time and proceed to stage 3.`**"],"metadata":{"id":"JT5Crwaxp2ZD"}},{"cell_type":"markdown","source":["`CODE`\n","* [Folder with clean copies](https://drive.google.com/drive/folders/186jZV_42xPERO6AjjorpctJu-5UZxkBt?usp=sharing ) of Lai-Lai-cut_v2.3 4 stages of automation\n","* [–ü–∞–ø–∫–∞ —Å —á–∏—Å—Ç—ã–º–∏ –∫–æ–ø–∏—è–º–∏](https://drive.google.com/drive/folders/186jZV_42xPERO6AjjorpctJu-5UZxkBt?usp=sharing ) Lai-Lai-Cut_v.2.3  - 4 —ç—Ç–∞–ø–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ.\n","\n","-------------\n","`INSTRUCTIONS`\n","* [Instructions ](https://docs.google.com/document/d/1_gZN3tDd3JB7WoKA8wNopDWpDy29ReKaZwa-nNC5Tng/edit) for naming and bunch processing. Details on input and output file names  for every stage.   \n","*[–§–∞–π–ª —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏](https://docs.google.com/document/d/1_gZN3tDd3JB7WoKA8wNopDWpDy29ReKaZwa-nNC5Tng/edit) –∫ –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–µ —Ñ–∞–π–ª–æ–≤, –∏ –∫ —Ä–∞–±–æ—Ç–µ —Å —á–µ—Ç—ã—Ä—å–º—è —ç—Ç–∞–ø–∞–º–∏, –≤\n","–∫–æ—Ç–æ—Ä–æ–º –º–æ–∂–Ω–æ —É—Ç–æ—á–Ω–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –≤–≤–æ–¥–∞ –∏ –≤—ã–≤–æ–¥–∞   "],"metadata":{"id":"9Dc-_753qFLP"}}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"19F_oEiV3sZ95_rGzQ9BT277F8GHMYtY2","timestamp":1730706254336},{"file_id":"1bQd6aY-5cVwxob1a7PcdzB6xGqEHAyjE","timestamp":1730706121376},{"file_id":"1Ei9_j-LdcHe64JNwEAlQcPITzDMXFNkG","timestamp":1723816817870},{"file_id":"1PZLyAw6AgpvGGBBvQBjQIrlv7f1be237","timestamp":1723617253534},{"file_id":"1G9Xtr_G5VtEapij8CF5lrXwAUkVadpvf","timestamp":1721941503803},{"file_id":"1DidJGv3DepWdeRyHpv_3wVOJb67rzgeo","timestamp":1721919232055},{"file_id":"1BzbvsXCU5nrhFRUSAnA2jSdP3_NuPhxe","timestamp":1721649760752},{"file_id":"1mMC_aJ8j3_Sf1PqIIounrn5RtKXcDu6u","timestamp":1720494623743},{"file_id":"1cR1izWp3o6q2RJt0tVz3JQH7VbTPAxIt","timestamp":1715868946389},{"file_id":"1SiapFB94-MTePmBpxal48jkN3BNovs98","timestamp":1715258089578},{"file_id":"https://gist.github.com/hbredin/049f2b629700bcea71324d2c1e7f8337#file-rich-transcription-ipynb","timestamp":1713595665059}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}