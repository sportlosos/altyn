{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sportlosos/altyn/blob/main/Copy_of_1_LaiLai_Cut_Get_transcripts_json_claude_v_2_6_debug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (!!)1. Run and restart this cell first"
      ],
      "metadata": {
        "id": "1ZhcEXZ6P35B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJDvkmXMSso-"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/m-bain/whisperx.git\n",
        "!pip install git+https://github.com/m-bain/whisperx.git --upgrade\n",
        "!pip install lightning_fabric\n",
        "#!!! you must run this cell, wait untill the system asks you to restart, then run again and proceed to the next cell"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ctranslate2==4.4.0"
      ],
      "metadata": {
        "id": "pIvd5iTeDLwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (!) 2 Mount G_DRIVE, agree for everything"
      ],
      "metadata": {
        "id": "Tw7ibZQRQq0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XUGrtS0pSw75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (!:) 3 Get API keys from pyannote huggingface . It's free. You only need to do it once for your account  "
      ],
      "metadata": {
        "id": "ND9Ot2k4QwjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# log in on Huggingface hub (where pretrained pyannote models are hosted)\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "password = userdata.get('HFpyannote1') # –í –ª–µ–≤–æ–π –ø–∞–Ω–µ–ª–∏ –µ—Å—Ç—å –∑–Ω–∞—á–æ–∫ –∫–ª—é—á–∞, –Ω—É–∂–Ω–æ –≤–∑—è—Ç—å –Ω–∞ Huggingface –∫–ª—é—á –∏ –Ω–∞–∑–≤–∞—Ç—å –µ–≥–æ HFpyannote1\n",
        "login(password)"
      ],
      "metadata": {
        "id": "pZ2ORSrpTSq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (!) 4 Provide a path to course folder in line 5\n",
        " (:)Checking for audio sourse (for our lectures it's usually m4a, if you have different audio/video file format just (!) specify in line 21 ) Here you can find naming instructions for root dir, enclosing folder, videos etc https://docs.google.com/document/d/1_gZN3tDd3JB7WoKA8wNopDWpDy29ReKaZwa-nNC5Tng/edit  "
      ],
      "metadata": {
        "id": "2clxu-50QOEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # üéµ Audio File Scanner\n",
        "#@markdown Enter the root directory path and file extension for your audio files, then run this cell.\n",
        "#@markdown ---\n",
        "#@markdown **Root Directory Path:**\n",
        "root_dir = \"/content/drive/MyDrive/Folder\" #@param {type:\"string\"}\n",
        "#@markdown **File Extension (include the dot):**\n",
        "file_extension = \".mp4\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "\n",
        "# Find all subdirectories in the root directory\n",
        "subdirectories = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
        "\n",
        "# Initialize an empty list to store the paths of all matching files\n",
        "audio_files = []\n",
        "\n",
        "# Loop through each subdirectory\n",
        "for subdir in subdirectories:\n",
        "    # Get a list of all files in the current subdirectory\n",
        "    files = os.listdir(subdir)\n",
        "\n",
        "    # Loop through each file in the current subdirectory\n",
        "    for file in files:\n",
        "        # Check if the file ends with the specified extension\n",
        "        if file.endswith(file_extension):\n",
        "            # Add the path of the file to the list\n",
        "            audio_files.append(os.path.join(subdir, file))\n",
        "\n",
        "file_count = len(audio_files)\n",
        "\n",
        "# Print the results\n",
        "print(f\"üîç **Scan Results:**\")\n",
        "print(f\"- Number of {file_extension} files found: {file_count}\")\n",
        "print(f\"- Root directory: {root_dir}\")\n",
        "print(\"\\nüìÅ **File Paths:**\")\n",
        "for path in audio_files:\n",
        "    print(f\"- {path}\")\n",
        "\n",
        "# To use the audio_files list in subsequent cells:\n",
        "# You can now use the 'audio_files' variable, which contains all the file paths"
      ],
      "metadata": {
        "id": "BfHrplJSUZ5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (:) 5 Getting mp3 files from whatever format - m4a, mp4, mov, wav  - any will do\n",
        "- pyannote wants MP3"
      ],
      "metadata": {
        "id": "mYrckd9ORE5E"
      }
    },
    {
      "source": [
        "import os\n",
        "\n",
        "# Ensure ffmpeg is available\n",
        "#!apt install ffmpeg\n",
        "\n",
        "# Function to convert audio or video to MP3\n",
        "def convert_to_mp3(input_file):\n",
        "    output_file = os.path.splitext(input_file)[0] + '.mp3'\n",
        "    # Use os.system for ffmpeg command execution\n",
        "    os.system(f'ffmpeg -i \"{input_file}\" -codec:a libmp3lame -qscale:a 2 \"{output_file}\"')\n",
        "    print(\"Conversion successful. Saved as:\", output_file)\n",
        "    return output_file\n",
        "\n",
        "# Use the audio_files list from the previous cell\n",
        "for audio_file in audio_files:  # Changed 'm4a_files' to 'audio_files'\n",
        "    # Check the file extension and act accordingly\n",
        "    file_extension = os.path.splitext(audio_file)[-1].lower()\n",
        "    supported_formats = ['.mp3', '.m4a', '.mp4', '.mov', '.wav']\n",
        "\n",
        "    if file_extension != '.mp3':\n",
        "        if file_extension in supported_formats:\n",
        "            audio_file = convert_to_mp3(input_file=audio_file)  # Convert and update audio_file to the MP3 path\n",
        "        else:\n",
        "            print(\"Unsupported file format.\")\n",
        "    else:\n",
        "        print(\"The file is already in MP3 format.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nUcByGfYPqd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (:) 6 Creating a list to store mp3 files"
      ],
      "metadata": {
        "id": "Ox4CIrQ6SNdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "import os\n",
        "\n",
        "# Initialize an empty list to store the paths of all .m4a files\n",
        "mp3_files = []\n",
        "\n",
        "# Loop through each subdirectory\n",
        "for subdir in subdirectories:\n",
        "    # Get a list of all files in the current subdirectory\n",
        "    files = os.listdir(subdir)\n",
        "\n",
        "    # Loop through each file in the current subdirectory\n",
        "    for file in files:\n",
        "        # Check if the file ends with \".m4a\"\n",
        "        if file.endswith(\".mp3\"):\n",
        "            # Add the path of the file to the list\n",
        "            mp3_files.append(os.path.join(subdir, file))\n",
        "\n",
        "# Print the list of all .m4a files\n",
        "print(mp3_files)"
      ],
      "metadata": {
        "id": "EvXhm7c6eota"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (:) 7 STARTING  Whisper and Pyannote to get transcription with word allignment and speaker diarisation,  (Time consuming, 1 hour of audio could take 5 to 20 min.)\n",
        "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫—É—Ä—Å–∞ –∏–∑ –≤–æ—Å—å–º–∏ –ª–µ–∫—Ü–∏–π –∑–∞–π–º–µ—Ç –±–æ–ª—å—à–µ —á–∞—Å–∞.  \n"
      ],
      "metadata": {
        "id": "iDsdKK0eS78c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisperx\n",
        "import gc\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "# Setup and model load\n",
        "device = \"cuda\"\n",
        "batch_size = 16\n",
        "compute_type = \"float16\"\n",
        "\n",
        "# Load model and audio, transcribe\n",
        "\n",
        "for audio_file in mp3_files:\n",
        "    model = whisperx.load_model(\"medium\", device, compute_type=compute_type)\n",
        "    audio = whisperx.load_audio(audio_file)\n",
        "\n",
        "    result = model.transcribe(audio, batch_size=batch_size)\n",
        "    print(\"Initial transcription:\", result[\"segments\"])\n",
        "\n",
        "# Alignment and Diarization\n",
        "    model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "    result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
        "    diarize_model = whisperx.DiarizationPipeline(device=device)\n",
        "    diarize_segments = diarize_model(audio)\n",
        "    result = whisperx.assign_word_speakers(diarize_segments, result)\n",
        "    print(\"Diarized segments with speaker labels:\", result[\"segments\"])\n",
        "\n",
        "# Save results to JSON\n",
        "    with open(f\"{audio_file}_transcription.json\", 'w', encoding='utf-8') as file:\n",
        "        json.dump(result[\"segments\"], file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    # Convert JSON data to CSV\n",
        "    with open(f\"{audio_file}_transcription.json\", 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    flat_data = []\n",
        "    for segment in data:\n",
        "        segment_start = segment.get('start', 'Unknown')\n",
        "        segment_end = segment.get('end', 'Unknown')\n",
        "        segment_text = segment.get('text', '')\n",
        "        segment_speaker = segment.get('speaker', 'Unknown')\n",
        "\n",
        "        if 'words' in segment and segment['words']:  # Ensure 'words' is not only present but also not empty\n",
        "            for word in segment['words']:\n",
        "                flat_data.append({\n",
        "                    'segment_start': segment_start,\n",
        "                    'segment_end': segment_end,\n",
        "                    'segment_text': segment_text,\n",
        "                    'segment_speaker': segment_speaker,\n",
        "                    'word': word.get('word', 'Unknown'),\n",
        "                    'word_start': word.get('start', 'Unknown'),\n",
        "                    'word_end': word.get('end', 'Unknown'),\n",
        "                    'word_score': word.get('score', 'Unknown'),\n",
        "                    'word_speaker': word.get('speaker', segment_speaker)\n",
        "                })\n",
        "        else:\n",
        "            # If there are no words, add a row representing the segment without word details\n",
        "            flat_data.append({\n",
        "                'segment_start': segment_start,\n",
        "                'segment_end': segment_end,\n",
        "                'segment_text': segment_text,\n",
        "                'segment_speaker': segment_speaker,\n",
        "                'word': 'No words',\n",
        "                'word_start': 'N/A',\n",
        "                'word_end': 'N/A',\n",
        "                'word_score': 'N/A',\n",
        "                'word_speaker': 'N/A'\n",
        "            })\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(flat_data)\n",
        "    df.to_csv(f\"{audio_file}_transcription.csv\", index=False)\n",
        "\n",
        "    print(f\"{audio_file}_transcription.csv has been successfully saved\")"
      ],
      "metadata": {
        "id": "PASU0u0KZo9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Go to stage `*`2.Bunch Clean for transcripts_Lai-Lai-Cut_v2.3 `*`\n",
        "–ü–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫ —ç—Ç–∞–ø—É –æ—á–∏—Å—Ç–∫–∏ –æ—Ç —Å–ª–æ–≤ –ø–∞—Ä–∞–∑–∏—Ç–æ–≤ (–≤–∞—à–∞ –ø—Ä–∏–Ω—Ü–µ—Å—Å–∞ –≤ –¥—Ä—É–≥–æ–º –∑–∞–º–∫–µ) **"
      ],
      "metadata": {
        "id": "SXU4GT5rJ3Wh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`CODE`\n",
        "* [Folder with clean copies](https://drive.google.com/drive/folders/186jZV_42xPERO6AjjorpctJu-5UZxkBt?usp=sharing ) of Lai-Lai-cut_v2.3 4 stages of automation\n",
        "* [–ü–∞–ø–∫–∞ —Å —á–∏—Å—Ç—ã–º–∏ –∫–æ–ø–∏—è–º–∏](https://drive.google.com/drive/folders/186jZV_42xPERO6AjjorpctJu-5UZxkBt?usp=sharing ) Lai-Lai-Cut_v.2.3  - 4 —ç—Ç–∞–ø–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ.\n",
        "\n",
        "-------------\n",
        "`INSTRUCTIONS`\n",
        "* [Instructions ](https://docs.google.com/document/d/1_gZN3tDd3JB7WoKA8wNopDWpDy29ReKaZwa-nNC5Tng/edit) for naming and bunch processing. Details on input and output file names  for every stage.   \n",
        "*[–§–∞–π–ª —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏](https://docs.google.com/document/d/1_gZN3tDd3JB7WoKA8wNopDWpDy29ReKaZwa-nNC5Tng/edit) –∫ –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–µ —Ñ–∞–π–ª–æ–≤, –∏ –∫ —Ä–∞–±–æ—Ç–µ —Å —á–µ—Ç—ã—Ä—å–º—è —ç—Ç–∞–ø–∞–º–∏, –≤\n",
        "–∫–æ—Ç–æ—Ä–æ–º –º–æ–∂–Ω–æ —É—Ç–æ—á–Ω–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –≤–≤–æ–¥–∞ –∏ –≤—ã–≤–æ–¥–∞   "
      ],
      "metadata": {
        "id": "AO1SgFlld-ZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`! Disconnect GPU time and proceed to stage 2.`**"
      ],
      "metadata": {
        "id": "YmxxC06kqXsp"
      }
    }
  ]
}